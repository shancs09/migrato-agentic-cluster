# Migrato Agentic Cluster Labeling Orchestrator

An AI-powered **document classification and cluster labeling assistant** built on **IBM watsonx** and foundation models (Meta Llama 3.2 90B Vision Instruct).  

This project orchestrates end-to-end cluster labeling workflows:

- Reads clustered document data
- Samples representative documents
- Infers intelligent cluster-level labels using LLMs
- Tracks similarity & status (Auto / Auto-Similar / ManualReview)
- Supports manual review and incremental labeling
- Exports clean summaries and CSV results for downstream use

---


## Architecture

![Overview](image.png)

## Agent Overview
**Agent name:** `AI_Cluster_Labeling_Agent`  
**Kind:** `native` (watsonx Orchestrate agent)  
**Model:** `watsonx/meta-llama/llama-3-2-90b-vision-instruct`  

The agent is designed to drive the following high-level flows:

### 1. Start Labeling

User says things like:

- â€œStart labelingâ€
- â€œLabel my clustersâ€
- â€œBegin labelingâ€

Agent calls:

- `GET /data/read`

Then behaves according to the state of the dataset:

- **Case 1 â€“ All clusters labeled**

  > All `<total_clusters>` clusters have already been labeled.  
  > There are no unlabeled clusters remaining.  
  >  
  > _Would you like to reset all labels and start over?_

- **Case 2 â€“ All clusters unlabeled**

  - Shows total clusters
  - Lists all unlabeled cluster IDs
  - Asks whether to:
    - Label all clusters, or
    - Start with a specific cluster

- **Case 3 â€“ Partial progress**

  - Shows labeled vs unlabeled counts
  - Lists unlabeled cluster IDs
  - Asks whether to:
    - Label all remaining clusters, or
    - Start with a specific cluster

---

### 2. Labeling Actions

The agent supports several labeling modes:

#### 2.1 Reset All Labels

User: â€œReset allâ€

- Call: `POST /data/reset?confirm=true`

Response to user:

> All labels have been reset.  
> Would you like to start labeling from the beginning?

---

#### 2.2 Label All Clusters

User: â€œLabel all clustersâ€

- Calls the **Infer Labels for All Cluster** tool:
  - Tool name in agent: `Infer_Labels_for_All_Cluster_4468MT`
  - Typically routes to an API that processes all unlabeled clusters

---

#### 2.3 Label a Single Cluster

User:

- â€œLabel cluster 5â€
- â€œRun cluster 3â€
- â€œLabel cluster with id 5â€

Agent calls:

- `POST /cluster/infersingle?cluster_id=<id>`

---

#### 2.4 Label Next N Clusters

User:

- â€œLabel next 3 clustersâ€
- â€œLabel 2 more clustersâ€

Agent calls:

- `POST /cluster/inferlimit?limit=N`

Rules:

- Only `limit` is sent
- **Do not** send `cluster_id` or `process_all`

---

### 3. Inference Tracking

For each inference operation, the agent (and backend) track:

- `cluster_id`
- `cluster_label`
- `status` â†’ `Auto` / `Auto-Similar` / `ManualReview`
- `similarity_score`
- `labels_used` (mapping of filename â†’ label)

These are used for internal decision-making and reporting.

---

### 4. Summary & Export Flows

#### 4.1 Summary (â€œShow me a summaryâ€)

User: â€œShow me a summaryâ€, â€œShow progressâ€, â€œShow all labels so farâ€

Agent calls:

- `GET /results/export/summary`

Agent then generates a **multi-block summary**:

1. **Overview**

   ```text
   You have <total_clusters> clusters in total.
   <labeled_clusters> clusters are labeled and <unlabeled_clusters> are unlabeled.
   Labeling coverage: <coverage_percent>%.
   Dominant label so far: "<dominant_label>" (<dominant_label_ratio>% of labeled clusters).

2. **Labeled Cluster Breakdown (Table)**

   Only if there are labeled clusters:

   | Label | Cluster Count | Cluster IDs |
   | ----- | ------------- | ----------- |

3. **Status Breakdown**

   | Status | Documents |
   | ------ | --------- |

4. **Download (Optional)**
   If `file_source` is present:

   ```text
   You can download the latest labeling file here: [Click here to download](file_source)
   ```

---

#### 4.2 Multi-Cluster Inference Summary

After processing multiple clusters (limit or process_all):

```text
<N> clusters have been processed. Here is the summary:
```

Followed by:

| Cluster ID | Label | Similarity | Notes |
| ---------- | ----- | ---------- | ----- |

End with:

```text
Would you like to label more clusters?
```

---

#### 4.3 Single-Cluster Summary

* **Newly labeled**

  ```text
  Cluster <id> has been labeled as "<label>".
  Would you like to label another cluster?
  ```

* **Already labeled**

  ```text
  Cluster <id> has already been labeled as "<label>".
  Would you like to label another cluster?
  ```

* **Manual review required**

  ```text
  Cluster <id> requires manual review.
  Would you like to provide a manual label now?
  ```

---

### 5. Manual Review Flow

If the backend marks any clusters with `ManualReview`:

```text
The following clusters require manual review: [cluster_ids]

Would you like to provide manual labels now?
```

The agent then:

* Accepts user-provided labels
* Allows re-running inference for those cluster IDs

---

### 6. Tooling Overview (Agent â†’ Backend)

The agent is configured with the following tools (as declared in the YAML):

* `read_data` â†’ `/data/read`
* `reset_labels` â†’ `/data/reset`
* `infer_labels_cluster_single` â†’ `/cluster/infersingle`
* `infer_labels_cluster_limit` â†’ `/cluster/inferlimit`
* `Infer_Labels_for_All_Cluster_4468MT` â†’ process all clusters
* `results_summary` â†’ `/results/export/summary`
* `export_results_csv` â†’ `/results/export?format=csv`
* `i__get_flow_status_intrinsic_tool__` â†’ internal flow status checks

> The exact HTTP methods and payloads are defined in the OpenAPI spec
> (e.g., `mig_cluster_label_openapi_v5.json`) and in the `tools/` Python modules.

---

## ğŸ—‚ï¸ Repository Structure

Typical structure of this repo:

```text
.
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ .gitignore
â”œâ”€â”€ .dockerignore
â”œâ”€â”€ env_sample               # Template for local .env (NOT committed)
â”œâ”€â”€ main.py                  # Entrypoint / orchestrator
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ agents_instruction_v2.yaml
â”œâ”€â”€ mig_cluster_label_openapi_v5.json
â”œâ”€â”€ AIClusterLabelingAgent_09dec_final.zip   # Importable agent package for watsonx
â”œâ”€â”€ tools/
â”‚   â”œâ”€â”€ cluster_labeler.py
â”‚   â”œâ”€â”€ data_utils.py
â”‚   â””â”€â”€ watsonx_utils.py
â”œâ”€â”€ database/
â”‚   â”œâ”€â”€ dbdump_restore.md
â”‚   â””â”€â”€ read_dump.py
â””â”€â”€ data/
    â””â”€â”€ (local CSVs only, ignored by Git)
```

> **Note:**
>
> * `.env` and all files under `data/` are intentionally **not** tracked (sensitive / local artifacts).
> * Use `env_sample` as a reference when creating your own `.env`.

---

## âš™ï¸ Prerequisites

* Python **3.10+** (ideally 3.11)
* `pip` / `pipenv` / `venv`
* Docker (optional, for containerized runs)
* IBM watsonx access + credentials (for calling foundation models / agents)
* Backend API service exposing the cluster labeling endpoints defined in the OpenAPI spec

---

## ğŸ” Environment Configuration

Create a `.env` file in the project root based on `env_sample`. Typical values might include:

* Watsonx / IAM credentials
* API base URLs (for `/data/*`, `/cluster/*`, `/results/*`)
* Model deployment IDs or project IDs

> âš ï¸ **Important:**
> `.env` is **not** tracked in Git.
> Never commit your real credentials.

---

## â–¶ï¸ Running Locally

1. **Clone the repository**

   ```bash
   git clone https://github.com/<your-username>/<repo-name>.git
   cd <repo-name>
   ```

2. **Create and activate a virtual environment (recommended)**

   ```bash
   python -m venv .venv
   source .venv/bin/activate      # macOS / Linux
   # .venv\Scripts\activate       # Windows (PowerShell)
   ```

3. **Install dependencies**

   ```bash
   pip install -r requirements.txt
   ```

4. **Configure environment**

   ```bash
   cp env_sample .env
   # Edit .env with your watsonx + backend settings
   ```

5. **Run the orchestrator**

   ```bash
   python main.py
   ```

   The exact behavior depends on how `main.py` is implemented (CLI / API server / worker).
   Typically this script will:

   * Initialize connections
   * Expose APIs or workflows used by the watsonx agent tools

---

## ğŸ³ Running with Docker

1. **Build the image**

   ```bash
   docker build -t migrato-agentic-cluster .
   ```

2. **Run the container**

   ```bash
   docker run --rm \
     --env-file .env \
     -p 8000:8000 \
     migrato-agentic-cluster
   ```

   Adjust the exposed port based on how `main.py` / your service is configured.

---

## ğŸ¤ Integrating with Watsonx Orchestrate

1. **Import the agent package**
   In watsonx Orchestrate, import:

   * `AIClusterLabelingAgent_09dec_final.zip`

2. **Configure tools**
   Map each tool in the agent:

   * `read_data`
   * `reset_labels`
   * `infer_labels_cluster_single`
   * `infer_labels_cluster_limit`
   * `Infer_Labels_for_All_Cluster_4468MT`
   * `results_summary`
   * `export_results_csv`

   to the corresponding backend endpoints (`/data/read`, `/cluster/infersingle`, etc.).

3. **Connect to your running backend**
   Ensure your backend (from this repo) is reachable from watsonx Orchestrate (network, auth, CORS, etc).

---

## ğŸ§ª Example Conversations

* **Start**

  > User: `Start labeling`
  > Agent â†’ `/data/read` â†’ responds with cluster coverage + options.

* **Label next 3 clusters**

  > User: `Label next 3 clusters`
  > Agent â†’ `/cluster/inferlimit?limit=3` â†’ returns table summary.

* **Show summary**

  > User: `Show me a summary of all the labels so far`
  > Agent â†’ `/results/export/summary` â†’ overview + tables + optional download link.

* **Download results**

  > User: `Download results`
  > Agent â†’ `/results/export?format=csv` â†’ clickable CSV link.

---

## ğŸ§­ Design Goals

* Avoid reprocessing already-labeled clusters
* Support incremental and resumable labeling
* Provide clear, structured summaries
* Enable manual review when similarity is low
* Expose clean download links (no backend filesystem paths)
* Keep instructions concise but deterministic for the agent

---
